{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "streamlit.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pickle\n",
        "uploaded_file = st.file_uploader(\"Choose a file\")\n",
        "import pandas as pd\n",
        "# import requests\n",
        "# import os\n",
        "# import csv\n",
        "# import numpy as np\n",
        "# import urllib.request\n",
        "# import gzip\n",
        "# import sys\n",
        "import spacy\n",
        "# import json\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import pkg_resources\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "import nltk\n",
        "\n",
        "def clean_data(df):\n",
        "\n",
        "    pd.options.mode.chained_assignment = None\n",
        "\n",
        "    print(\"******Cleaning Started*****\")\n",
        "\n",
        "    print(f'Shape of df before cleaning : {df.shape}')\n",
        "    df = df[df['Review'].notna()]\n",
        "    df['Review'] = df['Review'].str.replace(\"<br />\", \" \")\n",
        "    df['Review'] = df['Review'].str.replace(\"\\[?\\[.+?\\]?\\]\", \" \")\n",
        "    df['Review'] = df['Review'].str.replace(\"\\/{3,}\", \" \")\n",
        "    df['Review'] = df['Review'].str.replace(\"\\&\\#.+\\&\\#\\d+?;\", \" \")\n",
        "    df['Review'] = df['Review'].str.replace(\"\\d+\\&\\#\\d+?;\", \" \")\n",
        "    df['Review'] = df['Review'].str.replace(\"\\&\\#\\d+?;\", \" \")\n",
        "    # df['Review'] = df['Review'].str.replace(\"\\d+\", \"\")\n",
        "    # df['Review'] = df['Review'].str.replace(\"pros:\", \"\")\n",
        "    # df['Review'] = df['Review'].str.replace(\".pros:\", \"\")  \n",
        "    # df['Review'] = df['Review'].str.replace(\".pros\", \"\")    \n",
        "    df['Review'] = df['Review'].str.replace(\"sound quality\", \"soundquality\")\n",
        "    df['Review'] = df['Review'].str.replace(\"delivery quality\", \"deliveryquality\")\n",
        "    df['Review'] = df['Review'].str.replace(\"noise cancellation\", \"noisecancellation\")\n",
        "    df['Review'] = df['Review'].str.replace(\"battery life\", \"batterylife\")\n",
        "    df['Review'] = df['Review'].str.replace(\"product quality\", \"productquality\")\n",
        "    df['Review'] = df['Review'].str.replace(\"doesn't\", \"does not\")\n",
        "    df['Review'] = df['Review'].str.replace(\"don't\", \"do not\")\n",
        "    df['Review'] = df['Review'].str.replace(\"n't\", \"not\")\n",
        "    df['Review'] = df['Review'].str.replace(\"\\n\", \" \")\n",
        "    df['Review'] = df['Review'].str.replace(\".\", \" \")\n",
        "    #facial expressions\n",
        "    df['Review'] = df['Review'].str.replace(\"\\:\\|\", \"\")\n",
        "    df['Review'] = df['Review'].str.replace(\"\\:\\)\", \"\")\n",
        "    df['Review'] = df['Review'].str.replace(\"\\:\\(\", \"\")\n",
        "    df['Review'] = df['Review'].str.replace(\"\\:\\/\", \"\")\n",
        "    #replace multiple spaces with single space\n",
        "    df['Review'] = df['Review'].str.replace(\"\\s{2,}\", \" \")\n",
        "\n",
        "    df['Review'] = df['Review'].str.lower()\n",
        "    print(f'Shape of df after cleaning : {df.shape}')\n",
        "    print(\"******Cleaning Ended*****\")\n",
        "\n",
        "\n",
        "    return(df)\n",
        "\n",
        "prod_pronouns = ['it','this','they','these']\n",
        "\n",
        "\n",
        "#Dependency parser\n",
        "def apply_extraction(row,nlp,sid):\n",
        "\n",
        "    \n",
        "\n",
        "    doc=nlp(row)\n",
        "\n",
        "\n",
        "    ## FIRST RULE OF DEPENDANCY PARSE -\n",
        "    ## M - Sentiment modifier || A - Aspect\n",
        "    ## RULE = M is child of A with a relationshio of amod\n",
        "    rule1_pairs = []\n",
        "    for token in doc:\n",
        "        A = \"999999\"\n",
        "        M = \"999999\"\n",
        "        if token.dep_ == \"amod\" and not token.is_stop:\n",
        "            M = token.text\n",
        "            A = token.head.text\n",
        "\n",
        "            # add adverbial modifier of adjective (e.g. 'most comfortable headphones')\n",
        "            M_children = token.children\n",
        "            for child_m in M_children:\n",
        "                if(child_m.dep_ == \"advmod\"):\n",
        "                    M_hash = child_m.text\n",
        "                    M = M_hash + \" \" + M\n",
        "                    break\n",
        "\n",
        "            # negation in adjective, the \"no\" keyword is a 'det' of the noun (e.g. no interesting characters)\n",
        "            A_children = token.head.children\n",
        "            for child_a in A_children:\n",
        "                if(child_a.dep_ == \"det\" and child_a.text == 'no'):\n",
        "                    neg_prefix = 'not'\n",
        "                    M = neg_prefix + \" \" + M\n",
        "                    break\n",
        "\n",
        "        if(A != \"999999\" and M != \"999999\"):\n",
        "            rule1_pairs.append((A, M,sid.polarity_scores(token.text)['compound'],1))\n",
        "\n",
        "    ## SECOND RULE OF DEPENDANCY PARSE -\n",
        "    ## M - Sentiment modifier || A - Aspect\n",
        "    #Direct Object - A is a child of something with relationship of nsubj, while\n",
        "    # M is a child of the same something with relationship of dobj\n",
        "    #Assumption - A verb will have only one NSUBJ and DOBJ\n",
        "\n",
        "    rule2_pairs = []\n",
        "    for token in doc:\n",
        "        children = token.children\n",
        "        A = \"999999\"\n",
        "        M = \"999999\"\n",
        "        add_neg_pfx = False\n",
        "        for child in children :\n",
        "            if(child.dep_ == \"nsubj\" and not child.is_stop):\n",
        "                A = child.text\n",
        "                # check_spelling(child.text)\n",
        "\n",
        "            if((child.dep_ == \"dobj\" and child.pos_ == \"ADJ\") and not child.is_stop):\n",
        "                M = child.text\n",
        "                #check_spelling(child.text)\n",
        "\n",
        "            if(child.dep_ == \"neg\"):\n",
        "                neg_prefix = child.text\n",
        "                add_neg_pfx = True\n",
        "\n",
        "    if (add_neg_pfx and M != \"999999\"):\n",
        "        M = neg_prefix + \" \" + M\n",
        "\n",
        "        if(A != \"999999\" and M != \"999999\"):\n",
        "            rule2_pairs.append((A, M,sid.polarity_scores(M)['compound'],2))\n",
        "\n",
        "\n",
        "    ## THIRD RULE OF DEPENDANCY PARSE -\n",
        "    ## M - Sentiment modifier || A - Aspect\n",
        "    ## Adjectival Complement - A is a child of something with relationship of nsubj, while\n",
        "    ## M is a child of the same something with relationship of acomp\n",
        "    ## Assumption - A verb will have only one NSUBJ and DOBJ\n",
        "    ## \"The sound of the speakers would be better. The sound of the speakers could be better\" - handled using AUX dependency\n",
        "\n",
        "\n",
        "\n",
        "    rule3_pairs = []\n",
        "\n",
        "    for token in doc:\n",
        "\n",
        "        children = token.children\n",
        "        A = \"999999\"\n",
        "        M = \"999999\"\n",
        "        add_neg_pfx = False\n",
        "        for child in children :\n",
        "            if(child.dep_ == \"nsubj\" and not child.is_stop):\n",
        "                A = child.text\n",
        "                # check_spelling(child.text)\n",
        "\n",
        "            if(child.dep_ == \"acomp\" and not child.is_stop):\n",
        "                M = child.text\n",
        "\n",
        "            # example - 'this could have been better' -> (this, not better)\n",
        "            if(child.dep_ == \"aux\" and child.tag_ == \"MD\"):\n",
        "                neg_prefix = \"not\"\n",
        "                add_neg_pfx = True\n",
        "\n",
        "            if(child.dep_ == \"neg\"):\n",
        "                neg_prefix = child.text\n",
        "                add_neg_pfx = True\n",
        "\n",
        "        if (add_neg_pfx and M != \"999999\"):\n",
        "            M = neg_prefix + \" \" + M\n",
        "                #check_spelling(child.text)\n",
        "\n",
        "        if(A != \"999999\" and M != \"999999\"):\n",
        "            rule3_pairs.append((A, M, sid.polarity_scores(M)['compound'],3))\n",
        "\n",
        "    ## FOURTH RULE OF DEPENDANCY PARSE -\n",
        "    ## M - Sentiment modifier || A - Aspect\n",
        "\n",
        "    #Adverbial modifier to a passive verb - A is a child of something with relationship of nsubjpass, while\n",
        "    # M is a child of the same something with relationship of advmod\n",
        "\n",
        "    #Assumption - A verb will have only one NSUBJ and DOBJ\n",
        "\n",
        "    rule4_pairs = []\n",
        "    for token in doc:\n",
        "\n",
        "\n",
        "        children = token.children\n",
        "        A = \"999999\"\n",
        "        M = \"999999\"\n",
        "        add_neg_pfx = False\n",
        "        for child in children :\n",
        "            if((child.dep_ == \"nsubjpass\" or child.dep_ == \"nsubj\") and not child.is_stop):\n",
        "                A = child.text\n",
        "                # check_spelling(child.text)\n",
        "\n",
        "            if(child.dep_ == \"advmod\" and not child.is_stop):\n",
        "                M = child.text\n",
        "                M_children = child.children\n",
        "                for child_m in M_children:\n",
        "                    if(child_m.dep_ == \"advmod\"):\n",
        "                        M_hash = child_m.text\n",
        "                        M = M_hash + \" \" + child.text\n",
        "                        break\n",
        "                #check_spelling(child.text)\n",
        "\n",
        "            if(child.dep_ == \"neg\"):\n",
        "                neg_prefix = child.text\n",
        "                add_neg_pfx = True\n",
        "\n",
        "        if (add_neg_pfx and M != \"999999\"):\n",
        "            M = neg_prefix + \" \" + M\n",
        "\n",
        "        if(A != \"999999\" and M != \"999999\"):\n",
        "            rule4_pairs.append((A, M,sid.polarity_scores(M)['compound'],4)) # )\n",
        "\n",
        "\n",
        "    ## FIFTH RULE OF DEPENDANCY PARSE -\n",
        "    ## M - Sentiment modifier || A - Aspect\n",
        "\n",
        "    #Complement of a copular verb - A is a child of M with relationship of nsubj, while\n",
        "    # M has a child with relationship of cop\n",
        "\n",
        "    #Assumption - A verb will have only one NSUBJ and DOBJ\n",
        "\n",
        "    rule5_pairs = []\n",
        "    for token in doc:\n",
        "        children = token.children\n",
        "        A = \"999999\"\n",
        "        buf_var = \"999999\"\n",
        "        for child in children :\n",
        "            if(child.dep_ == \"nsubj\" and not child.is_stop):\n",
        "                A = child.text\n",
        "                # check_spelling(child.text)\n",
        "\n",
        "            if(child.dep_ == \"cop\" and not child.is_stop):\n",
        "                buf_var = child.text\n",
        "                #check_spelling(child.text)\n",
        "\n",
        "        if(A != \"999999\" and buf_var != \"999999\"):\n",
        "            rule5_pairs.append((A, token.text,sid.polarity_scores(token.text)['compound'],5))\n",
        "\n",
        "\n",
        "    ## SIXTH RULE OF DEPENDANCY PARSE -\n",
        "    ## M - Sentiment modifier || A - Aspect\n",
        "    ## Example - \"It ok\", \"ok\" is INTJ (interjections like bravo, great etc)\n",
        "\n",
        "\n",
        "    rule6_pairs = []\n",
        "    for token in doc:\n",
        "        children = token.children\n",
        "        A = \"999999\"\n",
        "        M = \"999999\"\n",
        "        if(token.pos_ == \"INTJ\" and not token.is_stop):\n",
        "            for child in children :\n",
        "                if(child.dep_ == \"nsubj\" and not child.is_stop):\n",
        "                    A = child.text\n",
        "                    M = token.text\n",
        "                    # check_spelling(child.text)\n",
        "\n",
        "        if(A != \"999999\" and M != \"999999\"):\n",
        "            rule6_pairs.append((A, M,sid.polarity_scores(M)['compound'],6))\n",
        "\n",
        "\n",
        "    ## SEVENTH RULE OF DEPENDANCY PARSE -\n",
        "    ## M - Sentiment modifier || A - Aspect\n",
        "    ## ATTR - link between a verb like 'be/seem/appear' and its complement\n",
        "    ## Example: 'this is garbage' -> (this, garbage)\n",
        "\n",
        "    rule7_pairs = []\n",
        "    for token in doc:\n",
        "        children = token.children\n",
        "        A = \"999999\"\n",
        "        M = \"999999\"\n",
        "        add_neg_pfx = False\n",
        "        for child in children :\n",
        "            if(child.dep_ == \"nsubj\" and not child.is_stop):\n",
        "                A = child.text\n",
        "                # check_spelling(child.text)\n",
        "\n",
        "            if((child.dep_ == \"attr\") and not child.is_stop):\n",
        "                M = child.text\n",
        "                #check_spelling(child.text)\n",
        "\n",
        "            if(child.dep_ == \"neg\"):\n",
        "                neg_prefix = child.text\n",
        "                add_neg_pfx = True\n",
        "\n",
        "        if (add_neg_pfx and M != \"999999\"):\n",
        "            M = neg_prefix + \" \" + M\n",
        "\n",
        "        if(A != \"999999\" and M != \"999999\"):\n",
        "            rule7_pairs.append((A, token.text,sid.polarity_scores(token.text)['compound'],7))\n",
        "\n",
        "\n",
        "\n",
        "    aspects = []\n",
        "\n",
        "    aspects = rule1_pairs + rule2_pairs + rule3_pairs +rule4_pairs +rule5_pairs + rule6_pairs + rule7_pairs\n",
        "\n",
        "    # replace all instances of \"it\", \"this\" and \"they\" with \"product\"\n",
        "    aspects = [(A,M,P,r) if A not in prod_pronouns else (\"product\",M,P,r) for A,M,P,r in aspects ]\n",
        "\n",
        "    dic = { \"aspect_pairs\" : aspects}\n",
        "\n",
        "    return dic\n",
        "\n",
        "#SymSpell\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=3)\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "def spell_check(aspect):\n",
        "  suggestions = sym_spell.lookup(aspect, Verbosity.CLOSEST)\n",
        "  for suggestion in suggestions:\n",
        "      aspect = suggestion.term\n",
        "      break;\n",
        "  return aspect\n",
        "\n",
        "#Main function\n",
        "st.title(\"Web Based Aspect-based sentiment analysis for earphone and headset\")\n",
        "input = st.text_input(\"Enter the review you want\")\n",
        "result = model.predict([input])\n",
        "btn = st.button(\"Predict\")\n",
        "if btn:\n",
        "  dic = (apply_extraction(input,nlp,sid)) #Dependency parsing\n",
        "  for x in dic['aspect_pairs']:\n",
        "    corrected_aspect = spell_check(x[0])\n",
        "    y = list(x)\n",
        "    y[0]=corrected_aspect\n",
        "    dic['aspect_pairs'][dic['aspect_pairs'].index(x)] = tuple(y)\n",
        "  for i in dic['aspect_pairs']:\n",
        "    if(i[0] in [\"item\",\"items\",\"quality\",\"sound\",\"soundquality\",\"design\",\"product\",\"connection\",\"looking\",\"call\",\"headphone\",\"headphones\",\"earphone\",\"earphones\",\n",
        "                  \"overall\",\"earpiece\",\"looks\",\"battery\",\"soundclarity\",\"features\",\"feature(s\",\"feature\",\"performance\",\"paired\",\"headset\",\"headsets\"\n",
        "                \"pairing\",\"something\",\"volume\",\"version\",\"earbuds\",\"earbud\",\"soundeffect\",\"effect\",\"playing\",\"control\",\"cast\",\"voice\",\"material\",\"piece\",\n",
        "                \"batterydrain\",\"mic\",\"color\",\"colour\",\"colors\",\"colours\",\"job\",\"jbl\",\"love\",\"casing\",\"use\",\"usage\",\"cover\",\"bluetooth\",\"clarity\",\"range\",\n",
        "                \"soundrange\",\"batch\",\"recommend\",\"texture\",\"portability\",\"case\",\"audio\",\"system\",\"device\",\"volume\",\"earbuds\",\"bass\",\"pitch\",\"tone\",\"noisecancellation\",\"microphone\",\"grade\",\"experience\",\n",
        "                \"delivery\",\"deliveryquality\",\"time\",\"deliverytime\",\"condition\",\"receive\",\"received\",\"packed\",\"packing\",\"package\",\"shipping\",\"value\",\"price\",\"buy\",\"purchase\",\"order\",\"deal\",\n",
        "                \"service\",\"staff\",\"seller\",\"reply\",\"follow\",\"followup\",\"gift\",'coordinating'and (ql !=\"Positive\" or ql!=\"Negative\")]):\n",
        "      st.text(\"Aspect: \"+i[0]+\" Sentiment: \"+i[1])\n",
        "  st.text(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyYivct9zb1A",
        "outputId": "31c48577-3261-419f-e79c-e0be90687e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeltaGenerator(_root_container=0, _provided_cursor=None, _parent=None, _block_type=None, _form_data=None)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}